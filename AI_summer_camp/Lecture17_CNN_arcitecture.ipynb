{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lecture17 CNN arcitecture.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF1vMa8KzM1q",
        "colab_type": "text"
      },
      "source": [
        "##0.개요\n",
        "https://ratsgo.github.io/deep%20learning/2017/10/09/CNNs/\n",
        "* AlexNet : 최초의 의미있는 성능의 CNN, 드랍아웃 기법의 표준화\n",
        "* GoogLeNet : 1x1 conv filter의 차원감소\n",
        "* ResNet : 기존의 층이 깊어질수록 역전파되는 그래디언트가 중간에 죽어 학습이 잘 되지 않는 문제(gradient vanishing)이 발생 -> residual block(**for skip connection**,forget gate를 도입해 이전 스텝의 그래디언트 정보를 좀더 잘 흐르게 만드려는 LSTM과 본질적으로 유사)\n",
        "![residual block](https://i.imgur.com/fse3Ntq.png)\n",
        "![unraveled view](https://i.imgur.com/CjLtXb0.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyA3IN0N-T5U",
        "colab_type": "text"
      },
      "source": [
        "##1.AlexNet\n",
        "* 우수한 정확도\n",
        "* 2개의 GPU를 사용한 구조(그 당시 3GB였기 때문)\n",
        "* 5개의 Conv 레이어, 3개의 FC 레이어\n",
        "* 3차원 구조의 convolution layer\n",
        " * RGB -> depth=3\n",
        "* zero-padding을 통해 얻은 이미지(227,227,3)->컨볼루션 커널(11,11,3,96) 통과 -> 출력 이미지 (55,55,96) ->풀링하여 927,27,96)->conv kernel(5,5,96,256) 통과->27,27,256 이미지 ->conv()\n",
        "\n",
        "* Lateral inhibition 현상에서 모델링하여 local response normalization->generalization 관점에서 더 좋은 정확도를 얻는것이 가능\n",
        "* Data augment, relu, dropout\n",
        "* 파라미터의 연산량을 줄이는 것이 모델 설계의 핵심\n",
        "\n",
        "\n",
        "##2.VGG16\n",
        "* 신경망의 깊이가 깊어짐\n",
        " * 장점 : 더 복잡한 문제의 해결\n",
        "  * 파라미터와 연산량의 증가, 그리고 overfitting\n",
        "\n",
        "* 파라미터의 숫자를 줄이는 방법\n",
        " * 5 x 5 conv 하나보다 3 x 3 두개가 낫다. 즉 여러개의 작은 conv filter(3 by 3 이하만) 쓰는 것이 적은 파라미터와 연산량으로 가능하다.\n",
        " * 3 x 3 2개 = 5 x 5 1개\n",
        " * 3 x 3 3개 = 7 x 7 1개\n",
        "\n",
        "* Vanishing gradient 문제를 해결하기 위해 11-layer의 학습 결과를 더 깊은 layer의 파라미터 초기화에 사용\n",
        "\n",
        "##3.GoogLeNet\n",
        "* 1 x 1 conv : 차원을 줄이는 방법. 비슷한 성질을 갖는 것들 묶음, 피쳐맵과 연산량을 줄이는데 특화\n",
        "* Inception module -> 1,3,5 square의 conv를 각각 수행\n",
        "* 보조분류기 사용\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnd6Yp7gC9E5",
        "colab_type": "text"
      },
      "source": [
        "##4.ResNet\n",
        "* 깊은 망 -> vanishing gradient\n",
        "* 입력과 출려의 차이값(F(x)=H(x)-x)를 찾는것. F(x)=0 최적의 경우 학습의 목표가 정해진 상태에서 학습\n",
        "* Bottleneck block : 두 종류의 residual block 사용, deep한 신경망 만들기 위해 3 layer 모델 사용\n",
        "* 5종류, 망 깊이와 정확도 비례\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XxcvmrAEfoC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}