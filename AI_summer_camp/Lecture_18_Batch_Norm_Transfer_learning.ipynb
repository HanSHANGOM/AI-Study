{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lecture 18 Batch Norm / Transfer learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEhWssMDF4gg",
        "colab_type": "text"
      },
      "source": [
        "##1.Batch Normalization\n",
        "\n",
        "* covariate shift : 학습하는 과정에서 이전 layer의 파라미터 변화로 인해 현재 layer의 분포가 바뀌는 현상\n",
        "* 학습과정에서는 분포가 비슷할수록 좋다.\n",
        "* 이는 건축현상에서의 휘어짐(Buckling)현상과 비슷하다. (뉴럴넷의 붕괴?를 방지하려면 BN을 해야함)\n",
        "* 이를 줄이기 위해서는 whitening : 입력을 평균 0, 분산 1로 바꾸는 것(정규분포화), 역전파와 무관하게 하면 안된다\n",
        "* whitening과 BN의 차이는 평균과 분산을 조절하는 과정이 신경망 안에 포함되어 있어 학습과정에서 평균과 분산을 조절\n",
        "\n",
        "####Batch Norm 방법\n",
        "* 학습 전체 과정에서 하는 것이 제일 좋지만 (역전파에 용이?) 파라미터의 업데이트가 Mini batch 단위로 진행되므로 Mini batch 단위로 batch Norm\n",
        "* 평균과 분산을 구한 후 정규화->정규화 이후에 scale(alpha)와 shift(beta)연산을 진행\n",
        "* 학습에서는 mini batch마다 scale와 shift를 구하고 그 값을 저장, 테스트 과정에서 그 평균을 사용\n",
        "* FC layer로만 이루어진 신경망에 적용하면 학습효과가 더 좋음\n",
        "\n",
        "![대체 텍스트](https://www.learnopencv.com/wp-content/uploads/2018/07/val_loss.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrDfrD_HnlYZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def BN_forward(x, gamma, beta, eps):\n",
        "    N, D = x.shape\n",
        "    mu = 1./N * np.sum(x, axis = 0)\n",
        "    xmu = x - mu\n",
        "    var = 1./N * np.sum(xmu**2, axis = 0)\n",
        "    sqrtvar = np.sqrt(var + eps)\n",
        "    ivar = 1./sqrtvar\n",
        "    xhat = xmu * ivar\n",
        "    out = (gamma * xhat) + beta\n",
        "    values = (xhat,gamma,xmu,ivar,sqrtvar,var,eps)\n",
        "    return out, values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhswveU7oPh9",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://shuuki4.files.wordpress.com/2016/01/bn1.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2F6tfo6oQzX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Backward 메서드입니다.\n",
        "def BN_backward(dout, cache):\n",
        "    xhat,gamma,xmu,ivar,sqrtvar,var,eps = cache\n",
        "    N,D = dout.shape\n",
        "    \n",
        "    dbeta = np.sum(dout, axis=0)\n",
        "    dgammax = dout #dr/dx= dr/dl * dl/dx ?\n",
        "    dgamma = np.sum(dgammax*xhat, axis=0)\n",
        "    \n",
        "\n",
        "    dxhat = dgammax*gamma\n",
        "    # (dl/dvar) 를 계산합니다.\n",
        "    divar = np.sum(dxhat*xmu, axis=0)\n",
        "    dsqrtvar = -1. /(sqrtvar**2) * divar\n",
        "    dvar = 0.5 * 1. /np.sqrt(var+eps) * dsqrtvar\n",
        "    \n",
        "    \n",
        "    dxmu1 = dxhat * ivar\n",
        "    dsq = 1. /N * np.ones((N,D)) * dvar\n",
        "    dxmu2 = 2 * xmu * dsq\n",
        "    \n",
        "    \n",
        "    dx1 = dxmu1+dxmu2\n",
        "    \n",
        "    \n",
        "    dmu = -1 * np.sum(dxmu1+dxmu2, axis=0)\n",
        "    dx2 = 1. /N * np.ones((N,D)) * dmu\n",
        "    \n",
        "    .\n",
        "    dx = dx1+dx2\n",
        "    \n",
        "    return dx, dgamma, dbeta\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuoBrGBJoSon",
        "colab_type": "text"
      },
      "source": [
        "[batch normalization 역전파 설명 링크](https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html)\n",
        "![대체 텍스트](https://kratzert.github.io/images/bn_backpass/BNcircuit.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iyvlbQToS7h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def ReLU(x):\n",
        "    return np.maximum(0,x)\n",
        "\n",
        "def histogram(x, title):\n",
        "    plt.hist(x)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Value')\n",
        "    plt.ylabel('Freqency')\n",
        "    plt.savefig('plot.png')\n",
        "    elice_utils.send_image(\"plot.png\")\n",
        "\n",
        "\n",
        "np.random.seed(50)\n",
        "x = np.random.rand(50,50)\n",
        "\n",
        "gamma, beta, epsilon = 1, 0, 10e-7\n",
        "histogram(x, 'Origin')\n",
        "\n",
        "\n",
        "out, cache = BN_forward(x,gamma,beta,epsilon)\n",
        "dx, dgamma, dbeta = BN_backward(out,cache)\n",
        "histogram(dx, 'Backward')\n",
        "\n",
        "out = ReLU(gamma)\n",
        "histogram(out,'Forward')\n",
        "\n",
        "\n",
        "\n",
        "   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT5X31usIWG9",
        "colab_type": "text"
      },
      "source": [
        "## 2.transfer learning\n",
        "\n",
        "* 잘 훈련된 모델을 사용하여 유사한 문제를 해결하는 방법\n",
        "* 학습을 위해서는 많은 데이터가 필요\n",
        "* 모델의 일부분을 변경했을 때 다시 전체를 학습하는 과정 필요 but 고사양 컴퓨터도 많은 시간 소요\n",
        "\n",
        "###동작 과정\n",
        "* 빅 데이터셋을 사용해 초기 신경망 모델 학습\n",
        "* 학습된 신경망 모델에 사용자 데이터셋을 사용하여 학습\n",
        "\n",
        "###전이학습 학습 방법\n",
        "![대체 텍스트](https://miro.medium.com/max/600/0*PvR62SNNcJZf3uGc.jpg)\n",
        "\n",
        "* 트레이닝 데이터의 크기와 트레이닝 데이터의 유사도에 따라 다르다.\n",
        "* 유사도도 놓고 데이터도 제일 많으면 좋음.\n",
        "* 그러나 대부분 데이터 유사도가 높아도 데이터셋의 크기가 작음\n",
        "*\n",
        "\n",
        "###지식 증류\n",
        "* Ensemble(앙상블)\n",
        " * 동일한 구조에서 Initialization을 다르게 여러번 학습하거나 다른 구조의 NN을 학습하고 나오는 결과물을 합치는 과정, 여러 모델을 돌리기 위한 컴퓨터 자원이 필요\n",
        "* Distilling the Ensemble Knowledge\n",
        " * 신경망의 지식 증류는 불필요한 파라미터가 사용되는 앙상블 모델에서 Eneralization 성능을 향상 시킬 수 있는 지식들을 분리하는 것\n",
        " * 이미지 분류에서 최종 출력은 Softmax를 사용\n",
        " * 앙상블 모델의 softmax 결과를 새로운 NN이 잘 전달 받는다면\n",
        "\n",
        "* 학습 과정은 Softmax 출력이 Label과 최대한 비슷해 지도록 Softmax cross entropy Loss를 최소화 하는 방식으로 학습하는데 이런 신경망에는 많은 정보가 담겨져 있음\n",
        "\n",
        "* Softer Softmax\n",
        "\n",
        "![대체 텍스트](https://bloglunit.files.wordpress.com/2018/03/20180322-research-seminar-google-slides-2018-03-22-17-24-15.png?w=226&h=149)\n",
        "\n",
        " 파라미터 T를 추가하여 T가 높을수록 기존보다 조금 더 soft한 확률분포를 얻을 수 있도록 함\n",
        " (마치 불순물 추출하는 과정에서 온도를 조절하는 것, 이를 조절함으로써 fine-tune을 더 잘 할 수 있게 해줄 수 있음)"
      ]
    }
  ]
}