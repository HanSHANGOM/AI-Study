K-means
입력,알고리즘
Expectation스텝(무게중심을 중심으로 클러스터링) Maximization스텝(클러스터의 무게중심을 업데이트)

-Local Minima 
-- 초기값 위치에 따라 원하는 결과가 나오지 않을 수 있음. 
--클러스터의 크기나 밀도가 다를 경우 원치않는 결과가 나올 수 있음.
--데이터의 분포가 특이한 경우에도

-실제 문제에 적용할 때는 여러번 클러스터링을 수행, 가장 빈번히 등장하는 군집에 할당하는 
majority voting 방법을 사용 
-다양한 k의 값을 시도해보고 cost function값의 변화량이 완만해지기 시작하는 순간을
최적이라고 판단.

KNN
-supervised. 
-범주 정보를 주변 이웃들을 가지고 추론.
-lazy모델 : 딱히 학습이라고 할만한 것이 없다?
-하이퍼파라미터 : 이웃 수, 거리측정방법(이웃 수가 작으면 오버피팅, 너무 크면 언더피팅)
-거리측정방법 (유클리드 distance, 맨해튼 distance)
유클리드는 일반적인 distance, 맨해튼은 각 셀의 총합(가로 쭉, 세로 쭉.)
마할라노비스(분산을 고려한 distance)도 있음. 블로그 참고. 
- 노이즈에 강하다.
그러나 데이터 분포마다 다 다르고 계산이 오래 걸린다.


Gaussian Mixture Model
-전체 데이터의 확률분포가 여러개의 정규분포의 조합으로 이루어져 있다고 가정하고 
각 분포에 속할 확률이 높은 데이터끼리 클러스터링 하는 방법
-다변수 정규분포를 가정할 수도 있다.
--요 분포를 따를 확률이 0.1 저 분포를 따를 확률이 0.4  이 분포를 따를 확률이 0.5 이런식
-기존 클러스터링으로 해결하지 못한 것들도 잘 동작.
-πk. (39페이지. https://untitledtblog.tistory.com/133 참조)
arg k max(k에 다라 비교하면서 제일 큰 수를 고른다.)
모델링할수 있는 범위가 넓어진다.

계층적 클러스터링(HC).
-개체들이 결합되는 순서를 나타내는 트리형태의 구조인 덴드로그램을 생성한후 적절한 수준에서
트리를 자르면 클러스터링이 완료됨.
-데이터 각 쌍에 대해서는 유사도를 측정할 수 있지만 평균이나 분산 등을 구할 수 없는 
경우에 사용하는 기법(예를 들면 문장)
-계산하려면 distance나 similarity가 이미 계산되어야 한다. 예를들면 거리행렬을 이미 구해놨다고 가정.
-거리가 가장 짧은 것에 해당하는 개체끼리 하나의 클러스터로 엮는다
->거리행렬을 다시 구해줌(min,max,group,between centroids 등의 distance)
->군집과 군집간 혹은 군집과 개체간 혹은 개체와 개체간의 거리 중 
가장 짧은 거리에 해당하는 것끼리 하나의 클러스터로 엮는다
(반복)
-그후 자를 부분을 선정.어디를 자르냐가 하이퍼파라미터. 덴드로그램에서 몇레벨에서 자를것인지 결정한다.
-50페이지 각 클러스터링의 특성 및 장단점.
-