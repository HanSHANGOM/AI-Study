{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lecture14 최적화.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdVuENzn5Wou",
        "colab_type": "text"
      },
      "source": [
        "## 1.딥러닝 학습의 문제점과 해결방법\n",
        "*   기울기 소실 - Sigmoid=> ReLU\n",
        "*   잘못된 초기값 설정\n",
        "* 과적합\n",
        "* 불안정한 학습과정\n",
        "\n",
        "\n",
        "###1-1.기울기 소실(Graident Vanishing)\n",
        "\n",
        "\n",
        "\n",
        "*   Sigmoid - \n",
        "*   ReLU - 0보다 작을 때 0을 줌.(문제점 : 기울기가 0이하에서 죽어버림. 그래서 그런 점을 보완해 Leaky ReLU를 사용함.)\n",
        "*  Leaky ReLU - 0보다 작을때 0이 아니라 약한 기울기를 줌.\n",
        "* tanh - 탄젠트의 역함수. \n",
        "\n",
        "\n",
        "###1-2.가중치 초기화\n",
        "\n",
        "* 가중치가 너무 커서 기울기 소실 문제가 발생하거나,입력층의 가중치를 0으로 초기화하면 역전파 때 두번째 층의 가중치가 모두 똑같이 갱신됨. bias는 0으로 하는 것이 좋음.\n",
        "*가중치를 표준 정규분포를 이용해 초기화->출력값이 0과 1에 몰림\n",
        "*표준편차를 낮춰 0.01로 한다->기울기 소실 문제 완화 but 아직부족\n",
        "####1)Xavier 초기화 방법\n",
        "*표준정규분포를 입력 개수의 제곱근으로 나누어 줌\n",
        "*Sigmoid와 같은 S자 함수의 경우 출력값이 정규분포 형태를 가져야 안정적으로 학습 가능(Sigmoid의 기울기값의 절댓값이 어느정도 작지 않은 구간에 많이 분포하므로)\n",
        "*ReLU함수는 Xavier 초기화가 부적합함.\n",
        "####2)He 초기화 방법\n",
        "*표준 정규 분포를 입력 개수 절반의 제곱근으로 나누어 줌\n",
        "w=np.random.randn(n_input,n_output)/sqrt(input개수//2)\n",
        "***\n",
        "\n",
        "## 2.최적화 알고리즘\n",
        "\n",
        "* 경사하강법\n",
        "*학습률\n",
        "\n",
        "###2-1.알고리즘의 종류\n",
        "* GD\n",
        "* SGD \n",
        "** Batch Gradient Descent = 손실함수를 계산할 때 전체 training set을 사용하는 것\n",
        "** mini-batch 학습, 시간을 절약하여 batch와 유사한 결과로 수렴함.\n",
        "* Momentum\n",
        "** 현재 Gradient를 통해 이동하는 방향과는 별개로, 과거에 이동했던 방향을 기억하여 그 방향으로 일정 정도를 추가적으로 이동하여 관성을 주는 방식\n",
        "γ*v(t-1)+η∇J(θ)\n",
        "* AdaGrad\n",
        "** 많이 변화하지 않은 변수들은 step size를 크게 하고, 많이 변화했던 \n",
        "* RMSProp\n",
        "** Adagrad의 단점을 해결하기 위해 합을 지수평균으로 대체\n",
        "** G가 무한정 커지지는 않으면서\n",
        "*Adam\n",
        "** Momentum+RMSProp\n",
        "\n",
        "28페이지 그림 참고,\n",
        "그 외에 NAG, Nadam 등\n",
        "\n",
        " \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SwaS4cltB1W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#keras optimizer 예제\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "word_num = 50\n",
        "data_num = 25000\n",
        "(train_data, train_labels), (test_data, test_labels) = keras.datasets.imdb.load_data(num_words=word_num)\n",
        "\n",
        "# 데이터 Shape을 맞춰주기 위한 sequence 함수를 불러옵니다.\n",
        "train_data = sequences_shaping(train_data, dimension=word_num)\n",
        "test_data = sequences_shaping(test_data, dimension=word_num)\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "\n",
        "optimizer_model = keras.Sequential([\n",
        "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='sgd',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy','sparse_categorical_crossentropy'])\n",
        "\n",
        "\n",
        "sgd =optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "rms=optimizers.RMSprop(lr=0.001)\n",
        "\n",
        "adadelta=optimizers.Adadelta(lr=0.1,rho=0.95,epsilon=None,decay=0.5)\n",
        "\n",
        "adagrad=optimizers.Adagrad(lr=0.1,epsilon=None,decay=0.0)\n",
        "\n",
        "adam=optimizers.Adam(lr=0.01,beta_1=0.9,beta_2=0.999)\n",
        "\n",
        "optimizer_model.compile(optimizer=rms,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy','sparse_categorical_crossentropy'])\n",
        "\n",
        "\n",
        "basic_history = basic_model.fit(train_data,train_labels,epochs=20,batch_size=500,validation_data=(test_data, test_labels), verbose=2)\n",
        "optimizer_history = optimizer_model.fit(train_data,train_labels,epochs=20,batch_size=500,validation_data=(test_data, test_labels), verbose=2)\n",
        "\n",
        "\n",
        "basic_score = model.evaluate(test_images, test_labels)\n",
        "optimizer_score = optimizer_model.evaluate(test_images, test_labels)\n",
        "\n",
        "\n",
        "name_history=[('Basic', basic_history),('Optimizer', optimizer_history)])\n",
        "for name, history in histories:\n",
        "  val = plt.plot(history.epoch, history.history['val_'+'sparse_categorical_crossentropy'],\n",
        "                '--', label=name.title()+' Val')\n",
        "    plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
        "             label=name.title()+' Train')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel(key.replace('_',' ').title())\n",
        "plt.legend()\n",
        "\n",
        "plt.xlim([0,max(history.epoch)])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}